<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Gary Lu&#39;s</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Gary Lu&#39;s">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Gary Lu&#39;s">
<meta property="og:locale">
<meta property="article:author" content="Gary">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Gary Lu's" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Gary Lu&#39;s</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Suche"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Suche"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-torch" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/09/05/torch/" class="article-date">
  <time class="dt-published" datetime="2021-09-05T19:14:19.465Z" itemprop="datePublished">2021-09-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="pytorch"><a href="#pytorch" class="headerlink" title="pytorch"></a>pytorch</h1><h2 id="0-0-import-amp-amp-version"><a href="#0-0-import-amp-amp-version" class="headerlink" title="0.0 import &amp;&amp; version"></a>0.0 import &amp;&amp; version</h2><p><em>导入包和版本查询</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="built_in">print</span>(torch.__version__)    <span class="comment"># 显示torch版本</span></span><br><span class="line"><span class="built_in">print</span>(torch.version.cuda)   <span class="comment"># 显示cuda版本</span></span><br><span class="line"><span class="built_in">print</span>(torch.backends.cudnn.version())</span><br><span class="line"><span class="built_in">print</span>(torch.cuda.get_device_name(<span class="number">0</span>))    <span class="comment"># 显示可用cuda设备</span></span><br></pre></td></tr></table></figure>

<h2 id="0-1-basis-random-seed"><a href="#0-1-basis-random-seed" class="headerlink" title="0.1 basis random seed"></a>0.1 basis random seed</h2><p>在硬件设备（CPU、GPU）不同时，完全的可复现性无法保证，即使随机种子相同。但是，在同一个设备上，应该保证可复现性。具体做法是，在程序开始的时候固定<code>torch</code>的随机种子，同时也把<code>numpy</code>的随机种子固定。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">0</span>)               <span class="comment"># numpy随机数种子</span></span><br><span class="line">torch.manual_seed(<span class="number">0</span>)</span><br><span class="line">torch.cuda.manual_seed_all(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br></pre></td></tr></table></figure>


<h2 id="0-2-argparse"><a href="#0-2-argparse" class="headerlink" title="0.2 argparse"></a>0.2 argparse</h2><p><code>argparse</code> 模块可以让人轻松编写用户友好的命令行接口。程序定义它需要的参数，然后 <code>argparse</code> 将弄清如何从 <code>sys.argv</code> 解析出那些参数。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">&#x27;Process some integers.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;integers&#x27;</span>, metavar=<span class="string">&#x27;N&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, nargs=<span class="string">&#x27;+&#x27;</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;an integer for the accumulator&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--sum&#x27;</span>, dest=<span class="string">&#x27;accumulate&#x27;</span>, action=<span class="string">&#x27;store_const&#x27;</span>,</span><br><span class="line">                    const=<span class="built_in">sum</span>, default=<span class="built_in">max</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;sum the integers (default: find the max)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">args = parser.parse_args()</span><br><span class="line"><span class="built_in">print</span>(args.accumulate(args.integers))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="1-GPU-setting"><a href="#1-GPU-setting" class="headerlink" title="1.GPU setting"></a>1.GPU setting</h2><p><em><strong>一张显卡 :</strong></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>) <span class="comment"># 设置训练device</span></span><br></pre></td></tr></table></figure>
<p><em><strong>多张显卡 :</strong></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;CUDA_VISIBLE_DEVICES&#x27;</span>] = <span class="string">&#x27;0,1&#x27;</span></span><br></pre></td></tr></table></figure>

<p><em><strong>在命令行运行代码时设置显卡：</strong></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span>,<span class="number">1</span> python train.py</span><br></pre></td></tr></table></figure>

<h2 id="2-Tensor"><a href="#2-Tensor" class="headerlink" title="2.Tensor"></a>2.Tensor</h2><p><em><strong>基本信息 :</strong></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.randn(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(tensor.<span class="built_in">type</span>())  <span class="comment"># 数据类型</span></span><br><span class="line"><span class="built_in">print</span>(tensor.size())  <span class="comment"># 张量的shape，是个元组tuple</span></span><br><span class="line"><span class="built_in">print</span>(tensor.dim())   <span class="comment"># 维度的数量</span></span><br></pre></td></tr></table></figure>

<p><em><strong>命名张量 :</strong></em></p>
<p>张量命名是一个非常有用的方法，这样可以方便地使用维度的名字来做索引或其他操作，大大提高了可读性、易用性，防止出错。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在PyTorch 1.3之前，需要使用注释</span></span><br><span class="line"><span class="comment"># Tensor[N, C, H, W]</span></span><br><span class="line">images = torch.randn(<span class="number">32</span>, <span class="number">3</span>, <span class="number">56</span>, <span class="number">56</span>)</span><br><span class="line">images.<span class="built_in">sum</span>(dim=<span class="number">1</span>)</span><br><span class="line">images.select(dim=<span class="number">1</span>, index=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># PyTorch 1.3之后</span></span><br><span class="line">NCHW = [‘N’, ‘C’, ‘H’, ‘W’]</span><br><span class="line">images = torch.randn(<span class="number">32</span>, <span class="number">3</span>, <span class="number">56</span>, <span class="number">56</span>, names=NCHW)</span><br><span class="line">images.<span class="built_in">sum</span>(<span class="string">&#x27;C&#x27;</span>)</span><br><span class="line">images.select(<span class="string">&#x27;C&#x27;</span>, index=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 也可以这么设置</span></span><br><span class="line">tensor = torch.rand(<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>,names=(<span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;N&#x27;</span>, <span class="string">&#x27;H&#x27;</span>, <span class="string">&#x27;W&#x27;</span>))</span><br><span class="line"><span class="comment"># 使用align_to可以对维度方便地排序</span></span><br><span class="line">tensor = tensor.align_to(<span class="string">&#x27;N&#x27;</span>, <span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;H&#x27;</span>, <span class="string">&#x27;W&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><em><strong>数据类型转换 :</strong></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置默认类型，pytorch 中的 FloatTensor 远远快于 DoubleTensor</span></span><br><span class="line">torch.set_default_tensor_type(torch.FloatTensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 类型转换</span></span><br><span class="line">tensor = tensor.cuda()</span><br><span class="line">tensor = tensor.cpu()</span><br><span class="line">tensor = tensor.<span class="built_in">float</span>()</span><br><span class="line">tensor = tensor.long()</span><br></pre></td></tr></table></figure>

<p><em><strong>torch.Tensor 与 np.ndarray 转换 :</strong></em></p>
<p>除了CharTensor，其他所有CPU上的张量都支持转换为numpy格式然后再转换回来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ndarray = tensor.cpu().numpy()</span><br><span class="line">tensor = torch.from_numpy(ndarray).<span class="built_in">float</span>()</span><br><span class="line">tensor = torch.from_numpy(ndarray.copy()).<span class="built_in">float</span>() <span class="comment"># If ndarray has negative stride.</span></span><br></pre></td></tr></table></figure>

<p><em><strong>Torch.tensor与PIL.Image转换 :</strong></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pytorch中的张量默认采用[N, C, H, W]的顺序，并且数据范围在[0,1]，需要进行转置和规范化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.Tensor -&gt; PIL.Image</span></span><br><span class="line">image = PIL.Image.fromarray(torch.clamp(tensor*<span class="number">255</span>, <span class="built_in">min</span>=<span class="number">0</span>, <span class="built_in">max</span>=<span class="number">255</span>).byte().permute(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>).cpu().numpy())</span><br><span class="line">image = torchvision.transforms.functional.to_pil_image(tensor)  <span class="comment"># Equivalently way</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># PIL.Image -&gt; torch.Tensor</span></span><br><span class="line">path = <span class="string">r&#x27;./figure.jpg&#x27;</span></span><br><span class="line">tensor = torch.from_numpy(np.asarray(PIL.Image.<span class="built_in">open</span>(path))).permute(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>).<span class="built_in">float</span>() / <span class="number">255</span></span><br><span class="line">tensor = torchvision.transforms.functional.to_tensor(PIL.Image.<span class="built_in">open</span>(path)) </span><br><span class="line"><span class="comment"># Equivalently way</span></span><br></pre></td></tr></table></figure>

<p><em><strong>np.ndarray与PIL.Image的转换 :</strong></em></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">image = PIL.Image.fromarray(ndarray.astype(np.uint8))</span><br><span class="line"></span><br><span class="line">ndarray = np.asarray(PIL.Image.open(path))</span><br></pre></td></tr></table></figure>

<p><em><strong>从只包含一个元素的张量中提取值 :</strong></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">value = torch.rand(<span class="number">1</span>).item()</span><br></pre></td></tr></table></figure>

<p><em><strong>张量形变 :</strong></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在将卷积层输入全连接层的情况下通常需要对张量做形变处理，</span></span><br><span class="line"><span class="comment"># 相比torch.view，torch.reshape可以自动处理输入张量不连续的情况。</span></span><br><span class="line">tensor = torch.rand(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">shape = (<span class="number">6</span>, <span class="number">4</span>)</span><br><span class="line">tensor = torch.reshape(tensor, shape)</span><br></pre></td></tr></table></figure>

<p><em><strong>打乱顺序 :</strong></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor = tensor[torch.randperm(tensor.size(<span class="number">0</span>))]  <span class="comment"># 打乱第一个维度</span></span><br></pre></td></tr></table></figure>

<p><em><strong>水平翻转 :</strong></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pytorch不支持tensor[::-1]这样的负步长操作，水平翻转可以通过张量索引实现</span></span><br><span class="line"><span class="comment"># 假设张量的维度为[N, D, H, W].</span></span><br><span class="line">tensor = tensor[:,:,:,torch.arange(tensor.size(<span class="number">3</span>) - <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>).long()]</span><br></pre></td></tr></table></figure>

<p><em><strong>复制张量 :</strong></em></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># Operation                 |  New/Shared memory | Still in computation graph |</span><br><span class="line">tensor.clone()            # |        New         |          Yes               |</span><br><span class="line">tensor.detach()           # |      Shared        |          No                |</span><br><span class="line">tensor.detach.clone()()   # |        New         |          No                |</span><br></pre></td></tr></table></figure>

<p><em><strong>张量拼接 :</strong></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">注意torch.cat和torch.stack的区别在于torch.cat沿着给定的维度拼接，</span></span><br><span class="line"><span class="string">而torch.stack会新增一维。例如当参数是3个10x5的张量，torch.cat的结果是30x5的张量，</span></span><br><span class="line"><span class="string">而torch.stack的结果是3x10x5的张量。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">tensor = torch.cat(list_of_tensors, dim=<span class="number">0</span>)</span><br><span class="line">tensor = torch.stack(list_of_tensors, dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p><em><strong>将整数标签转为one-hot编码 :</strong></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pytorch的标记默认从0开始</span></span><br><span class="line">tensor = torch.tensor([<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line">N = tensor.size(<span class="number">0</span>)</span><br><span class="line">num_classes = <span class="number">4</span></span><br><span class="line">one_hot = torch.zeros(N, num_classes).long()</span><br><span class="line">one_hot.scatter_(dim=<span class="number">1</span>, index=torch.unsqueeze(tensor, dim=<span class="number">1</span>), src=torch.ones(N, num_classes).long())</span><br></pre></td></tr></table></figure>

<p><em><strong>得到非零元素 :</strong></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.nonzero(tensor)               <span class="comment"># index of non-zero elements</span></span><br><span class="line">torch.nonzero(tensor==<span class="number">0</span>)            <span class="comment"># index of zero elements</span></span><br><span class="line">torch.nonzero(tensor).size(<span class="number">0</span>)       <span class="comment"># number of non-zero elements</span></span><br><span class="line">torch.nonzero(tensor == <span class="number">0</span>).size(<span class="number">0</span>)  <span class="comment"># number of zero elements</span></span><br></pre></td></tr></table></figure>

<p><em><strong>判断两个张量相等 :</strong></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.allclose(tensor1, tensor2)  <span class="comment"># float tensor</span></span><br><span class="line">torch.equal(tensor1, tensor2)     <span class="comment"># int tensor</span></span><br></pre></td></tr></table></figure>

<p><em><strong>张量扩展:</strong></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Expand tensor of shape 64*512 to shape 64*512*7*7.</span></span><br><span class="line">tensor = torch.rand(<span class="number">64</span>,<span class="number">512</span>)</span><br><span class="line">torch.reshape(tensor, (<span class="number">64</span>, <span class="number">512</span>, <span class="number">1</span>, <span class="number">1</span>)).expand(<span class="number">64</span>, <span class="number">512</span>, <span class="number">7</span>, <span class="number">7</span>)</span><br></pre></td></tr></table></figure>

<p><em><strong>矩阵乘法 :</strong></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Matrix multiplcation: (m*n) * (n*p) * -&gt; (m*p).</span></span><br><span class="line">result = torch.mm(tensor1, tensor2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Batch matrix multiplication: (b*m*n) * (b*n*p) -&gt; (b*m*p)</span></span><br><span class="line">result = torch.bmm(tensor1, tensor2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Element-wise multiplication.</span></span><br><span class="line">result = tensor1 * tensor2</span><br></pre></td></tr></table></figure>

<p><em><strong>计算两组数据之间的两两欧式距离 :</strong></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#利用broadcast机制</span></span><br><span class="line">dist = torch.sqrt(torch.<span class="built_in">sum</span>((X1[:,<span class="literal">None</span>,:] - X2) ** <span class="number">2</span>, dim=<span class="number">2</span>))</span><br></pre></td></tr></table></figure>

<h2 id="3-模型定义和操作"><a href="#3-模型定义和操作" class="headerlink" title="3.模型定义和操作"></a>3.模型定义和操作</h2><h1 id="一个简单两层卷积网络的示例"><a href="#一个简单两层卷积网络的示例" class="headerlink" title="一个简单两层卷积网络的示例"></a>一个简单两层卷积网络的示例</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># convolutional neural network (2 convolutional layers)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConvNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_classes=<span class="number">10</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ConvNet, self).__init__()</span><br><span class="line">        self.layer1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">16</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line">        self.layer2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">32</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line">        self.fc = nn.Linear(<span class="number">7</span>*<span class="number">7</span>*<span class="number">32</span>, num_classes)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        out = self.layer1(x)</span><br><span class="line">        out = self.layer2(out)</span><br><span class="line">        out = out.reshape(out.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        out = self.fc(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = ConvNet(num_classes).to(device)</span><br></pre></td></tr></table></figure>

<p><em><strong>双线性汇合（bilinear pooling） :</strong></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X = torch.reshape(N, D, H * W)                        <span class="comment"># Assume X has shape N*D*H*W</span></span><br><span class="line">X = torch.bmm(X, torch.transpose(X, <span class="number">1</span>, <span class="number">2</span>)) / (H * W)  <span class="comment"># Bilinear pooling</span></span><br><span class="line"><span class="keyword">assert</span> X.size() == (N, D, D)</span><br><span class="line">X = torch.reshape(X, (N, D * D))</span><br><span class="line">X = torch.sign(X) * torch.sqrt(torch.<span class="built_in">abs</span>(X) + <span class="number">1e-5</span>)   <span class="comment"># Signed-sqrt normalization</span></span><br><span class="line">X = torch.nn.functional.normalize(X)                  <span class="comment"># L2 normalization</span></span><br></pre></td></tr></table></figure>

<p><em><strong>多卡同步 BN（Batch normalization）:</strong></em></p>
<p>当使用 torch.nn.DataParallel 将代码运行在多张 GPU 卡上时，PyTorch 的 BN 层默认操作是各卡上数据独立地计算均值和标准差，同步 BN 使用所有卡上的数据一起计算 BN 层的均值和标准差，缓解了当批量大小（batch size）比较小时对均值和标准差估计不准的情况，是在目标检测等任务中一个有效的提升性能的技巧。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sync_bn = torch.nn.SyncBatchNorm(num_features, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p><em><strong>将已有网络的所有BN层改为同步BN层 :</strong></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convertBNtoSyncBN</span>(<span class="params">module, process_group=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Recursively replace all BN layers to SyncBN layer.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        module[torch.nn.Module]. Network</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, torch.nn.modules.batchnorm._BatchNorm):</span><br><span class="line">        sync_bn = torch.nn.SyncBatchNorm(module.num_features, module.eps, module.momentum, </span><br><span class="line">                                         module.affine, module.track_running_stats, process_group)</span><br><span class="line">        sync_bn.running_mean = module.running_mean</span><br><span class="line">        sync_bn.running_var = module.running_var</span><br><span class="line">        <span class="keyword">if</span> module.affine:</span><br><span class="line">            sync_bn.weight = module.weight.clone().detach()</span><br><span class="line">            sync_bn.bias = module.bias.clone().detach()</span><br><span class="line">        <span class="keyword">return</span> sync_bn</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> name, child_module <span class="keyword">in</span> module.named_children():</span><br><span class="line">            <span class="built_in">setattr</span>(module, name) = convert_syncbn_model(child_module, process_group=process_group))</span><br><span class="line">        <span class="keyword">return</span> module</span><br></pre></td></tr></table></figure>

<p><em><strong>类似 BN 滑动平均 :</strong></em></p>
<p>如果要实现类似 BN 滑动平均的操作，在 forward 函数中要使用原地（inplace）操作给滑动平均赋值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BN</span>(<span class="params">torch.nn.Module</span>)</span></span><br><span class="line"><span class="class">    <span class="title">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        ...</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;running_mean&#x27;</span>, torch.zeros(num_features))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        ...</span><br><span class="line">        self.running_mean += momentum * (current - self.running_mean)</span><br></pre></td></tr></table></figure>

<p><em><strong>计算模型整体参数量 :</strong></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">num_parameters = <span class="built_in">sum</span>(torch.numel(parameter) <span class="keyword">for</span> parameter <span class="keyword">in</span> model.parameters())</span><br></pre></td></tr></table></figure>

<p><em><strong>查看网络中的参数 :</strong></em></p>
<p>可以通过model.state_dict()或者model.named_parameters()函数查看现在的全部可训练参数（包括通过继承得到的父类中的参数）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">params = <span class="built_in">list</span>(model.named_parameters())</span><br><span class="line">(name, param) = params[<span class="number">28</span>]</span><br><span class="line"><span class="built_in">print</span>(name)</span><br><span class="line"><span class="built_in">print</span>(param.grad)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-------------------------------------------------&#x27;</span>)</span><br><span class="line">(name2, param2) = params[<span class="number">29</span>]</span><br><span class="line"><span class="built_in">print</span>(name2)</span><br><span class="line"><span class="built_in">print</span>(param2.grad)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;----------------------------------------------------&#x27;</span>)</span><br><span class="line">(name1, param1) = params[<span class="number">30</span>]</span><br><span class="line"><span class="built_in">print</span>(name1)</span><br><span class="line"><span class="built_in">print</span>(param1.grad)</span><br></pre></td></tr></table></figure>

<p><em><strong>模型可视化（使用pytorchviz）:</strong></em></p>
<p>链接：<a target="_blank" rel="noopener" href="https://github.com/szagoruyko/pytorchviz">https://github.com/szagoruyko/pytorchviz</a></p>
<p>类似 Keras 的 model.summary() 输出模型信息（使用pytorch-summary ）<br>链接：<a target="_blank" rel="noopener" href="https://github.com/sksq96/pytorch-summary">https://github.com/sksq96/pytorch-summary</a></p>
<p>注意 model.modules() 和 model.children() 的区别：model.modules() 会迭代地遍历模型的所有子层，而 model.children() 只会遍历模型下的一层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Common practise for initialization.</span></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> model.modules():</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(layer, torch.nn.Conv2d):</span><br><span class="line">        torch.nn.init.kaiming_normal_(layer.weight, mode=<span class="string">&#x27;fan_out&#x27;</span>,</span><br><span class="line">                                      nonlinearity=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> layer.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            torch.nn.init.constant_(layer.bias, val=<span class="number">0.0</span>)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(layer, torch.nn.BatchNorm2d):</span><br><span class="line">        torch.nn.init.constant_(layer.weight, val=<span class="number">1.0</span>)</span><br><span class="line">        torch.nn.init.constant_(layer.bias, val=<span class="number">0.0</span>)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(layer, torch.nn.Linear):</span><br><span class="line">        torch.nn.init.xavier_normal_(layer.weight)</span><br><span class="line">        <span class="keyword">if</span> layer.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            torch.nn.init.constant_(layer.bias, val=<span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialization with given tensor.</span></span><br><span class="line">layer.weight = torch.nn.Parameter(tensor)</span><br></pre></td></tr></table></figure>

<p><em><strong>提取模型中的某一层 :</strong></em></p>
<p><code>modules()</code>会返回模型中所有模块的迭代器，它能够访问到最内层，比如<code>self.layer1.conv1</code>这个模块，还有一个与它们相对应的是<code>name_children()</code>属性以及<code>named_modules()</code>,这两个不仅会返回模块的迭代器，还会返回网络层的名字。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 取模型中的前两层</span></span><br><span class="line">new_model = nn.Sequential(*<span class="built_in">list</span>(model.children())[:<span class="number">2</span>] </span><br><span class="line"><span class="comment"># 如果希望提取出模型中的所有卷积层，可以像下面这样操作：</span></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> model.named_modules():</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(layer[<span class="number">1</span>],nn.Conv2d):</span><br><span class="line">         conv_model.add_module(layer[<span class="number">0</span>],layer[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<p><em><strong>部分层使用预训练模型 :</strong></em></p>
<p>注意如果保存的模型是 <code>torch.nn.DataParallel</code>，则当前的模型也需要是</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;model.pth&#x27;</span>), strict=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p><em><strong>将在 GPU 保存的模型加载到 CPU :</strong></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;model.pth&#x27;</span>, map_location=<span class="string">&#x27;cpu&#x27;</span>))</span><br></pre></td></tr></table></figure>

<p><em><strong>导入另一个模型的相同部分到新的模型 :</strong></em></p>
<p>模型导入参数时，如果两个模型结构不一致，则直接导入参数会报错。用下面方法可以把另一个模型的相同的部分导入到新的模型中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model_new代表新的模型</span></span><br><span class="line"><span class="comment"># model_saved代表其他模型，比如用torch.load导入的已保存的模型</span></span><br><span class="line">model_new_dict = model_new.state_dict()</span><br><span class="line">model_common_dict = &#123;k:v <span class="keyword">for</span> k, v <span class="keyword">in</span> model_saved.items() <span class="keyword">if</span> k <span class="keyword">in</span> model_new_dict.keys()&#125;</span><br><span class="line">model_new_dict.update(model_common_dict)</span><br><span class="line">model_new.load_state_dict(model_new_dict)</span><br></pre></td></tr></table></figure>

<h2 id="4-数据处理"><a href="#4-数据处理" class="headerlink" title="4.数据处理"></a>4.数据处理</h2><p>计算数据集的均值和标准差</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_mean_and_std</span>(<span class="params">dataset</span>):</span></span><br><span class="line">    <span class="comment"># 输入PyTorch的dataset，输出均值和标准差</span></span><br><span class="line">    mean_r = <span class="number">0</span></span><br><span class="line">    mean_g = <span class="number">0</span></span><br><span class="line">    mean_b = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> img, _ <span class="keyword">in</span> dataset:</span><br><span class="line">        img = np.asarray(img) <span class="comment"># change PIL Image to numpy array</span></span><br><span class="line">        mean_r += np.mean(img[:, :, <span class="number">0</span>])</span><br><span class="line">        mean_g += np.mean(img[:, :, <span class="number">1</span>])</span><br><span class="line">        mean_b += np.mean(img[:, :, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">    mean_r /= <span class="built_in">len</span>(dataset)</span><br><span class="line">    mean_g /= <span class="built_in">len</span>(dataset)</span><br><span class="line">    mean_b /= <span class="built_in">len</span>(dataset)</span><br><span class="line"></span><br><span class="line">    diff_r = <span class="number">0</span></span><br><span class="line">    diff_g = <span class="number">0</span></span><br><span class="line">    diff_b = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    N = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> img, _ <span class="keyword">in</span> dataset:</span><br><span class="line">        img = np.asarray(img)</span><br><span class="line"></span><br><span class="line">        diff_r += np.<span class="built_in">sum</span>(np.power(img[:, :, <span class="number">0</span>] - mean_r, <span class="number">2</span>))</span><br><span class="line">        diff_g += np.<span class="built_in">sum</span>(np.power(img[:, :, <span class="number">1</span>] - mean_g, <span class="number">2</span>))</span><br><span class="line">        diff_b += np.<span class="built_in">sum</span>(np.power(img[:, :, <span class="number">2</span>] - mean_b, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        N += np.prod(img[:, :, <span class="number">0</span>].shape)</span><br><span class="line"></span><br><span class="line">    std_r = np.sqrt(diff_r / N)</span><br><span class="line">    std_g = np.sqrt(diff_g / N)</span><br><span class="line">    std_b = np.sqrt(diff_b / N)</span><br><span class="line"></span><br><span class="line">    mean = (mean_r.item() / <span class="number">255.0</span>, mean_g.item() / <span class="number">255.0</span>, mean_b.item() / <span class="number">255.0</span>)</span><br><span class="line">    std = (std_r.item() / <span class="number">255.0</span>, std_g.item() / <span class="number">255.0</span>, std_b.item() / <span class="number">255.0</span>)</span><br><span class="line">    <span class="keyword">return</span> mean, std</span><br></pre></td></tr></table></figure>

<p><em><strong>常用训练和验证数据预处理 :</strong></em></p>
<p>其中 ToTensor 操作会将 PIL.Image 或形状为 H×W×D，数值范围为 [0, 255] 的 np.ndarray 转换为形状为 D×H×W，数值范围为 [0.0, 1.0] 的 torch.Tensor。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">train_transform = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.RandomResizedCrop(size=<span class="number">224</span>,</span><br><span class="line">                                             scale=(<span class="number">0.08</span>, <span class="number">1.0</span>)),</span><br><span class="line">    torchvision.transforms.RandomHorizontalFlip(),</span><br><span class="line">    torchvision.transforms.ToTensor(),</span><br><span class="line">    torchvision.transforms.Normalize(mean=(<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>),</span><br><span class="line">                                     std=(<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>)),</span><br><span class="line"> ])</span><br><span class="line"> val_transform = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.Resize(<span class="number">256</span>),</span><br><span class="line">    torchvision.transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">    torchvision.transforms.ToTensor(),</span><br><span class="line">    torchvision.transforms.Normalize(mean=(<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>),</span><br><span class="line">                                     std=(<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>)),</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<h2 id="5-模型训练和测试"><a href="#5-模型训练和测试" class="headerlink" title="5. 模型训练和测试"></a>5. 模型训练和测试</h2><p><em><strong>分类模型训练代码 :</strong></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loss and optimizer</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model</span></span><br><span class="line">total_step = <span class="built_in">len</span>(train_loader)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> i ,(images, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        images = images.to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Forward pass</span></span><br><span class="line">        outputs = model(images)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Backward and optimizer</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (i+<span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Epoch: [&#123;&#125;/&#123;&#125;], Step: [&#123;&#125;/&#123;&#125;], Loss: &#123;&#125;&#x27;</span></span><br><span class="line">                  .<span class="built_in">format</span>(epoch+<span class="number">1</span>, num_epochs, i+<span class="number">1</span>, total_step, loss.item()))</span><br></pre></td></tr></table></figure>

<p><em><strong>分类模型测试代码 :</strong></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test the model</span></span><br><span class="line">model.<span class="built_in">eval</span>()  <span class="comment"># eval mode(batch norm uses moving mean/variance </span></span><br><span class="line">              <span class="comment">#instead of mini-batch mean/variance)</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> test_loader:</span><br><span class="line">        images = images.to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line">        outputs = model(images)</span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs.data, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line">        </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Test accuracy of the model on the 10000 test images: &#123;&#125; %&#x27;</span>.<span class="built_in">format</span>(<span class="number">100</span> * correct / total))</span><br></pre></td></tr></table></figure>

<p><em><strong>自定义loss:</strong></em></p>
<p>继承torch.nn.Module类写自己的loss</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyLoss</span>(<span class="params">torch.nn.Moudle</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyLoss, self).__init__()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, y</span>):</span></span><br><span class="line">        loss = torch.mean((x - y) ** <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>

<p><em><strong>标签平滑（label smoothing） :</strong></em></p>
<p>写一个label_smoothing.py的文件，然后在训练代码里引用，用LSR代替交叉熵损失即可。label_smoothing.py内容如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSR</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, e=<span class="number">0.1</span>, reduction=<span class="string">&#x27;mean&#x27;</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.log_softmax = nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line">        self.e = e</span><br><span class="line">        self.reduction = reduction</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_one_hot</span>(<span class="params">self, labels, classes, value=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            Convert labels to one hot vectors</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            labels: torch tensor in format [label1, label2, label3, ...]</span></span><br><span class="line"><span class="string">            classes: int, number of classes</span></span><br><span class="line"><span class="string">            value: label value in one hot vector, default to 1</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            return one hot format labels in shape [batchsize, classes]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        one_hot = torch.zeros(labels.size(<span class="number">0</span>), classes)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#labels and value_added  size must match</span></span><br><span class="line">        labels = labels.view(labels.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        value_added = torch.Tensor(labels.size(<span class="number">0</span>), <span class="number">1</span>).fill_(value)</span><br><span class="line"></span><br><span class="line">        value_added = value_added.to(labels.device)</span><br><span class="line">        one_hot = one_hot.to(labels.device)</span><br><span class="line"></span><br><span class="line">        one_hot.scatter_add_(<span class="number">1</span>, labels, value_added)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> one_hot</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_smooth_label</span>(<span class="params">self, target, length, smooth_factor</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;convert targets to one-hot format, and smooth</span></span><br><span class="line"><span class="string">        them.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            target: target in form with [label1, label2, label_batchsize]</span></span><br><span class="line"><span class="string">            length: length of one-hot format(number of classes)</span></span><br><span class="line"><span class="string">            smooth_factor: smooth factor for label smooth</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            smoothed labels in one hot format</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        one_hot = self._one_hot(target, length, value=<span class="number">1</span> - smooth_factor)</span><br><span class="line">        one_hot += smooth_factor / (length - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> one_hot.to(target.device)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, target</span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> x.size(<span class="number">0</span>) != target.size(<span class="number">0</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&#x27;Expected input batchsize (&#123;&#125;) to match target batch_size(&#123;&#125;)&#x27;</span></span><br><span class="line">                    .<span class="built_in">format</span>(x.size(<span class="number">0</span>), target.size(<span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> x.dim() &lt; <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&#x27;Expected input tensor to have least 2 dimensions(got &#123;&#125;)&#x27;</span></span><br><span class="line">                    .<span class="built_in">format</span>(x.size(<span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> x.dim() != <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&#x27;Only 2 dimension tensor are implemented, (got &#123;&#125;)&#x27;</span></span><br><span class="line">                    .<span class="built_in">format</span>(x.size()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        smoothed_target = self._smooth_label(target, x.size(<span class="number">1</span>), self.e)</span><br><span class="line">        x = self.log_softmax(x)</span><br><span class="line">        loss = torch.<span class="built_in">sum</span>(- x * smoothed_target, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.reduction == <span class="string">&#x27;none&#x27;</span>:</span><br><span class="line">            <span class="keyword">return</span> loss</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">elif</span> self.reduction == <span class="string">&#x27;sum&#x27;</span>:</span><br><span class="line">            <span class="keyword">return</span> torch.<span class="built_in">sum</span>(loss)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">elif</span> self.reduction == <span class="string">&#x27;mean&#x27;</span>:</span><br><span class="line">            <span class="keyword">return</span> torch.mean(loss)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&#x27;unrecognized option, expect reduction to be one of none, mean, sum&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>或者直接在训练文件里做label smoothing</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">    images, labels = images.cuda(), labels.cuda()</span><br><span class="line">    N = labels.size(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># C is the number of classes.</span></span><br><span class="line">    smoothed_labels = torch.full(size=(N, C), fill_value=<span class="number">0.1</span> / (C - <span class="number">1</span>)).cuda()</span><br><span class="line">    smoothed_labels.scatter_(dim=<span class="number">1</span>, index=torch.unsqueeze(labels, dim=<span class="number">1</span>), value=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">    score = model(images)</span><br><span class="line">    log_prob = torch.nn.functional.log_softmax(score, dim=<span class="number">1</span>)</span><br><span class="line">    loss = -torch.<span class="built_in">sum</span>(log_prob * smoothed_labels) / N</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">Mixup训练</span><br><span class="line">beta_distribution = torch.distributions.beta.Beta(alpha, alpha)</span><br><span class="line"><span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">    images, labels = images.cuda(), labels.cuda()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mixup images and labels.</span></span><br><span class="line">    lambda_ = beta_distribution.sample([]).item()</span><br><span class="line">    index = torch.randperm(images.size(<span class="number">0</span>)).cuda()</span><br><span class="line">    mixed_images = lambda_ * images + (<span class="number">1</span> - lambda_) * images[index, :]</span><br><span class="line">    label_a, label_b = labels, labels[index]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mixup loss.</span></span><br><span class="line">    scores = model(mixed_images)</span><br><span class="line">    loss = (lambda_ * loss_function(scores, label_a)</span><br><span class="line">            + (<span class="number">1</span> - lambda_) * loss_function(scores, label_b))</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>

<p><em><strong>L1 正则化 :</strong></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">l1_regularization = torch.nn.L1Loss(reduction=<span class="string">&#x27;sum&#x27;</span>)</span><br><span class="line">loss = ...  <span class="comment"># Standard cross-entropy loss</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    loss += torch.<span class="built_in">sum</span>(torch.<span class="built_in">abs</span>(param))</span><br><span class="line">loss.backward()</span><br></pre></td></tr></table></figure>

<p><em><strong>不对偏置项进行权重衰减（weight decay）:</strong></em></p>
<p>pytorch里的weight decay相当于l2正则</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">bias_list = (param <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> name[-<span class="number">4</span>:] == <span class="string">&#x27;bias&#x27;</span>)</span><br><span class="line">others_list = (param <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> name[-<span class="number">4</span>:] != <span class="string">&#x27;bias&#x27;</span>)</span><br><span class="line">parameters = [&#123;<span class="string">&#x27;parameters&#x27;</span>: bias_list, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>&#125;,                </span><br><span class="line">              &#123;<span class="string">&#x27;parameters&#x27;</span>: others_list&#125;]</span><br><span class="line">optimizer = torch.optim.SGD(parameters, lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">1e-4</span>)</span><br></pre></td></tr></table></figure>

<p><em><strong>梯度裁剪（gradient clipping）:</strong></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=<span class="number">20</span>)</span><br></pre></td></tr></table></figure>

<p><em><strong>得到当前学习率 :</strong></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># If there is one global learning rate (which is the common case).</span></span><br><span class="line">lr = <span class="built_in">next</span>(<span class="built_in">iter</span>(optimizer.param_groups))[<span class="string">&#x27;lr&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># If there are multiple learning rates for different layers.</span></span><br><span class="line">all_lr = []</span><br><span class="line"><span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">    all_lr.append(param_group[<span class="string">&#x27;lr&#x27;</span>])</span><br></pre></td></tr></table></figure>

<p>另一种方法，在一个batch训练代码里，当前的<code>lr</code>是<code>optimizer.param_groups[0][&#39;lr&#39;]</code></p>
<p><em><strong>学习率衰减:</strong></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Reduce learning rate when validation accuarcy plateau.</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=<span class="string">&#x27;max&#x27;</span>, patience=<span class="number">5</span>, verbose=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">80</span>):</span><br><span class="line">    train(...)</span><br><span class="line">    val(...)</span><br><span class="line">    scheduler.step(val_acc)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Cosine annealing learning rate.</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=<span class="number">80</span>)</span><br><span class="line"><span class="comment"># Reduce learning rate by 10 at given epochs.</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[<span class="number">50</span>, <span class="number">70</span>], gamma=<span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">80</span>):</span><br><span class="line">    scheduler.step()    </span><br><span class="line">    train(...)</span><br><span class="line">    val(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Learning rate warmup by 10 epochs.</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=<span class="keyword">lambda</span> t: t / <span class="number">10</span>)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">10</span>):</span><br><span class="line">    scheduler.step()</span><br><span class="line">    train(...)</span><br><span class="line">    val(...)</span><br></pre></td></tr></table></figure>

<p><em><strong>优化器链式更新 :</strong></em></p>
<p>从1.4版本开始，torch.optim.lr_scheduler 支持链式更新（chaining），即用户可以定义两个 schedulers，并交替在训练中使用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> SGD</span><br><span class="line"><span class="keyword">from</span> torch.optim.lr_scheduler <span class="keyword">import</span> ExponentialLR, StepLR</span><br><span class="line">model = [torch.nn.Parameter(torch.randn(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>))]</span><br><span class="line">optimizer = SGD(model, <span class="number">0.1</span>)</span><br><span class="line">scheduler1 = ExponentialLR(optimizer, gamma=<span class="number">0.9</span>)</span><br><span class="line">scheduler2 = StepLR(optimizer, step_size=<span class="number">3</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">    <span class="built_in">print</span>(epoch, scheduler2.get_last_lr()[<span class="number">0</span>])</span><br><span class="line">    optimizer.step()</span><br><span class="line">    scheduler1.step()</span><br><span class="line">    scheduler2.step()</span><br></pre></td></tr></table></figure>

<p><em><strong>模型训练可视化 :</strong></em></p>
<p>PyTorch可以使用tensorboard来可视化训练过程。</p>
<p>安装和运行TensorBoard。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install tensorboard</span><br><span class="line">tensorboard --logdir=runs</span><br></pre></td></tr></table></figure>

<p>使用SummaryWriter类来收集和可视化相应的数据，放了方便查看，可以使用不同的文件夹，比如<code>&#39;Loss/train&#39;</code>和<code>&#39;Loss/test&#39;</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n_iter <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;Loss/train&#x27;</span>, np.random.random(), n_iter)</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;Loss/test&#x27;</span>, np.random.random(), n_iter)</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;Accuracy/train&#x27;</span>, np.random.random(), n_iter)</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;Accuracy/test&#x27;</span>, np.random.random(), n_iter)</span><br></pre></td></tr></table></figure>

<p><em><strong>保存与加载断点 :</strong></em></p>
<p>注意为了能够恢复训练，我们需要同时保存模型和优化器的状态，以及当前的训练轮数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">start_epoch = <span class="number">0</span></span><br><span class="line"><span class="comment"># Load checkpoint.</span></span><br><span class="line"><span class="keyword">if</span> resume: <span class="comment"># resume为参数，第一次训练时设为0，中断再训练时设为1</span></span><br><span class="line">    model_path = os.path.join(<span class="string">&#x27;model&#x27;</span>, <span class="string">&#x27;best_checkpoint.pth.tar&#x27;</span>)</span><br><span class="line">    <span class="keyword">assert</span> os.path.isfile(model_path)</span><br><span class="line">    checkpoint = torch.load(model_path)</span><br><span class="line">    best_acc = checkpoint[<span class="string">&#x27;best_acc&#x27;</span>]</span><br><span class="line">    start_epoch = checkpoint[<span class="string">&#x27;epoch&#x27;</span>]</span><br><span class="line">    model.load_state_dict(checkpoint[<span class="string">&#x27;model&#x27;</span>])</span><br><span class="line">    optimizer.load_state_dict(checkpoint[<span class="string">&#x27;optimizer&#x27;</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Load checkpoint at epoch &#123;&#125;.&#x27;</span>.<span class="built_in">format</span>(start_epoch))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Best accuracy so far &#123;&#125;.&#x27;</span>.<span class="built_in">format</span>(best_acc))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(start_epoch, num_epochs): </span><br><span class="line">    ... </span><br><span class="line"></span><br><span class="line">    <span class="comment"># Test the model</span></span><br><span class="line">    ...</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># save checkpoint</span></span><br><span class="line">    is_best = current_acc &gt; best_acc</span><br><span class="line">    best_acc = <span class="built_in">max</span>(current_acc, best_acc)</span><br><span class="line">    checkpoint = &#123;</span><br><span class="line">        <span class="string">&#x27;best_acc&#x27;</span>: best_acc,</span><br><span class="line">        <span class="string">&#x27;epoch&#x27;</span>: epoch + <span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;model&#x27;</span>: model.state_dict(),</span><br><span class="line">        <span class="string">&#x27;optimizer&#x27;</span>: optimizer.state_dict(),</span><br><span class="line">    &#125;</span><br><span class="line">    model_path = os.path.join(<span class="string">&#x27;model&#x27;</span>, <span class="string">&#x27;checkpoint.pth.tar&#x27;</span>)</span><br><span class="line">    best_model_path = os.path.join(<span class="string">&#x27;model&#x27;</span>, <span class="string">&#x27;best_checkpoint.pth.tar&#x27;</span>)</span><br><span class="line">    torch.save(checkpoint, model_path)</span><br><span class="line">    <span class="keyword">if</span> is_best:</span><br><span class="line">        shutil.copy(model_path, best_model_path)</span><br></pre></td></tr></table></figure>

<h2 id="6-其他注意事项"><a href="#6-其他注意事项" class="headerlink" title="6. 其他注意事项"></a>6. 其他注意事项</h2><ul>
<li><p>不要使用太大的线性层。因为<code>nn.Linear(m,n)</code>使用的是O(mn)的内存，线性层太大很容易超出现有显存。</p>
</li>
<li><p>不要在太长的序列上使用RNN。因为RNN反向传播使用的是BPTT算法，其需要的内存和输入序列的长度呈线性关系。</p>
</li>
<li><p>model(x) 前用 <code>model.train()</code> 和 <code>model.eval()</code> 切换网络状态。</p>
</li>
<li><p>不需要计算梯度的代码块用 <code>with torch.no_grad()</code> 包含起来。</p>
</li>
<li><p><code>model.eval()</code> 和 <code>torch.no_grad()</code> 的区别在于，<code>model.eval()</code> 是将网络切换为测试状态，例如 BN 和dropout在训练和测试阶段使用不同的计算方法。<code>torch.no_grad()</code> 是关闭 PyTorch 张量的自动求导机制，以减少存储使用和加速计算，得到的结果无法进行 <code>loss.backward()</code>。</p>
</li>
<li><p><code>model.zero_grad()</code>会把整个模型的参数的梯度都归零, 而<code>optimizer.zero_grad()</code>只会把传入其中的参数的梯度归零.</p>
</li>
<li><p><code>torch.nn.CrossEntropyLoss</code> 的输入不需要经过 Softmax。<code>torch.nn.CrossEntropyLoss</code> 等价于 <code>torch.nn.functional.log_softmax</code> + <code>torch.nn.NLLLoss</code>。</p>
</li>
<li><p><code>loss.backward()</code> 前用 <code>optimizer.zero_grad()</code> 清除累积梯度。</p>
</li>
<li><p><code>torch.utils.data.DataLoader</code> 中尽量设置 <code>pin_memory=True</code>，对特别小的数据集如 MNIST 设置 <code>pin_memory=False</code> 反而更快一些。num_workers 的设置需要在实验中找到最快的取值。</p>
</li>
<li><p>用 del 及时删除不用的中间变量，节约 GPU 存储。</p>
</li>
<li><p>使用 inplace 操作可节约 GPU 存储，如</p>
</li>
</ul>
<p><code>x = torch.nn.functional.relu(x, inplace=True)</code></p>
<ul>
<li><p>减少 CPU 和 GPU 之间的数据传输。例如如果你想知道一个 epoch 中每个 mini-batch 的 loss 和准确率，先将它们累积在 GPU 中等一个 epoch 结束之后一起传输回 CPU 会比每个 mini-batch 都进行一次 GPU 到 CPU 的传输更快。</p>
</li>
<li><p>使用半精度浮点数 <code>half()</code> 会有一定的速度提升，具体效率依赖于 GPU 型号。需要小心数值精度过低带来的稳定性问题。</p>
</li>
<li><p>时常使用 <code>assert tensor.size() == (N, D, H, W)</code> 作为调试手段，确保张量维度和你设想中一致。</p>
</li>
<li><p>除了标记 y 外，尽量少使用一维张量，使用 n*1 的二维张量代替，可以避免一些意想不到的一维张量计算结果。</p>
</li>
<li><p>统计代码各部分耗时</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.autograd.profiler.profile(enabled=<span class="literal">True</span>, use_cuda=<span class="literal">False</span>) <span class="keyword">as</span> profile:</span><br><span class="line">    ...</span><br><span class="line"><span class="built_in">print</span>(profile)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者在命令行运行</span></span><br><span class="line">python -m torch.utils.bottleneck main.py</span><br></pre></td></tr></table></figure></li>
<li><p>使用<code>TorchSnooper</code>来调试PyTorch代码，程序在执行的时候，就会自动 print 出来每一行的执行结果的 tensor 的形状、数据类型、设备、是否需要梯度的信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pip install torchsnooper</span></span><br><span class="line"><span class="keyword">import</span> torchsnooper</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于函数，使用修饰器</span></span><br><span class="line"><span class="meta">@torchsnooper.snoop()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果不是函数，使用 with 语句来激活 TorchSnooper，把训练的那个循环装进 with 语句中去。</span></span><br><span class="line"><span class="keyword">with</span> torchsnooper.snoop():</span><br><span class="line">    原本的代码</span><br></pre></td></tr></table></figure></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/09/05/torch/" data-id="ckt7lgkim00009g7k5kl09gq1" data-title="" class="article-share-link">Teilen</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-hello-world" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/09/05/hello-world/" class="article-date">
  <time class="dt-published" datetime="2021-09-05T18:21:23.408Z" itemprop="datePublished">2021-09-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/09/05/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/09/05/hello-world/" data-id="ckt7lcnrv0000wo7kfww01dp0" data-title="Hello World" class="article-share-link">Teilen</a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archiv</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/09/">September 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">letzter Beitrag</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/09/05/torch/">(no title)</a>
          </li>
        
          <li>
            <a href="/2021/09/05/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 Gary<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>